{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_news_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7OxL13AZwuo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e3d5b6c9-52c9-4058-f966-d319c5eb509f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.externals import joblib\n",
        "from google.colab import drive\n",
        "from google.colab import drive,files\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1I-TvQRegW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Model to Training Data \n",
        "\n",
        "  Forward Function\n",
        "Take length of Embedding and Dim of it and Create Embedding Layer by torch Framework\n",
        "Create Neural Network Layer take 903 input and return 256 as output layer\n",
        "then bass output to non-activation function layer then add dropout to output\n",
        "bass output to Linear take 256 and return number of class 1 and then add sigmoid to output\n",
        "\n",
        "  Conv Function \n",
        "Take inputs ( self, inputs layer[batch size of training inputs*Featuers] (64,1440) )\n",
        "\n",
        "conv inputs layer from [64,1440] to [64,903]\n",
        "\n",
        "first 300s number repersent the vector sentence(Statement) of Embedding\n",
        "\n",
        "301 add Similarity of Statement Featuer (first [410] numbers of orignal input) and \n",
        "  Subject Featuer (second [30] numbers of orignal input)\n",
        "\n",
        "from 302 to 602 add number repersent the vector sentence(Subject) of Embedding\n",
        "\n",
        "602 add Similarity of Subject Featuer ([30] numbers of orignal input) and \n",
        "  Justification Featuer (second [1000] numbers of orignal input)\n",
        "\n",
        "from 602 to 902 add number repersent the vector sentence(Justifaction) of Embedding\n",
        "\n",
        "903 add Similarity of Justification Featuer ([100] numbers of orignal input) and \n",
        "  Statement Featuer (second [410] numbers of orignal input)\n",
        "\n",
        "and return [64,903]\n",
        "\n",
        "Then Pass output of Conv to Forward\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class NN(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, vocab_size,batch_size, embedding_dim,lens,word_embedding,hidden_dim):\n",
        "\n",
        "    super(NN, self).__init__()\n",
        "\n",
        "    self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.hidden_dim=hidden_dim\n",
        "    self.batch_size=batch_size\n",
        "    self.lens=lens\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)   \n",
        "    self.embedding.weight.data.copy_(word_embedding)\n",
        "\n",
        "    self.lstm = nn.LSTM(3*self.embedding_dim+3, self.hidden_dim, 2, dropout=0.7, batch_first=True)\n",
        "    self.fc = nn.Linear(self.hidden_dim,1)\n",
        "    self.dropout=nn.Dropout(0.7)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x,hidden):\n",
        "\n",
        "    batch = x.size(0)\n",
        "\n",
        "    x = self.conv(x)\n",
        "\n",
        "\n",
        "    lstm_out, hidden = self.lstm(x, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1,self.hidden_dim)\n",
        "    out = self.dropout(lstm_out)\n",
        "\n",
        "    out=self.sig(self.fc(out))\n",
        "    sig_out = out.view(batch, -1)\n",
        "    sig_out = sig_out[:, -1]\n",
        "    return sig_out, hidden\n",
        "\n",
        "    \n",
        "  def conv(self,x):\n",
        "    \n",
        "    bacth=len(x)\n",
        "    ret=torch.zeros((bacth,3*self.embedding_dim+3)).to(self.device)\n",
        "    st=torch.zeros(self.embedding_dim).to(self.device)\n",
        "    su=torch.zeros(self.embedding_dim).to(self.device)\n",
        "    ju=torch.zeros(self.embedding_dim).to(self.device)\n",
        "\n",
        "\n",
        "    \n",
        "    for i in range(bacth):\n",
        "      \n",
        "      st=self.embedding(x[i][0:self.lens[0]]).sum(dim=0)/(x[i][0:self.lens[0]]!=0).sum()\n",
        "      su=self.embedding(x[i][self.lens[0]:self.lens[1]]).sum(dim=0)/(x[i][self.lens[0]:self.lens[1]]!=0).sum()\n",
        "      ju=self.embedding(x[i][self.lens[1]:self.lens[2]]).sum(dim=0)/(x[i][self.lens[1]:self.lens[2]]!=0).sum()\n",
        "      \n",
        "      ret[i][:self.embedding_dim]=st\n",
        "      ret[i][self.embedding_dim]=F.cosine_similarity(st,su,dim=0)\n",
        "\n",
        "      ret[i][self.embedding_dim+1:2*self.embedding_dim+1]=su\n",
        "      ret[i][self.embedding_dim*2+1]=F.cosine_similarity(su,ju,dim=0)\n",
        "\n",
        "      ret[i][2*self.embedding_dim+2:3*self.embedding_dim+2]=ju\n",
        "      \n",
        "      ret[i][self.embedding_dim*3+2]=F.cosine_similarity(st,ju,dim=0)\n",
        "    return ret.view(1,bacth,self.embedding_dim*3+3)\n",
        "\n",
        "\n",
        "  def init_hidden(self):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(2, 1, self.hidden_dim).zero_().to(self.device),\n",
        "                  weight.new(2, 1, self.hidden_dim).zero_().to(self.device))\n",
        "    return hidden\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahp_h11F1ViY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class preprocess():\n",
        "  def __init__(self):\n",
        "\n",
        "    super(preprocess, self).__init__()\n",
        "    self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    self.all_statements,self.all_justifications=self.load_data()\n",
        "\n",
        "    self.word_to_index, self.index_to_word, self.word_to_vec_map =self.read_glove_vecs('/content/drive/My Drive/Datasets/Word Embedding/glove.6B.300d.txt')\n",
        "    self.word_embedding=self.pretrained_embedding_layer(self.word_to_vec_map,self.word_to_index)\n",
        "    self.model=NN(len(self.word_to_index)+1,32,300,[410,440,1440],torch.from_numpy(self.word_embedding),256)\n",
        "    self.model.load_state_dict(torch.load('/content/drive/My Drive/model.pt',map_location=torch.device(self.device)))\n",
        "    self.model.to(self.device)\n",
        "\n",
        "  \"\"\"\n",
        "     Clean Text \n",
        " \n",
        "  \"\"\"\n",
        "\n",
        "  def load_data(self):\n",
        "    cols=['index','ID','label','statement','subject','speaker','speaker_job','state','party','barely_true',\n",
        "        'false','half_true','mostly_true','pants_on_fire','context','justification']\n",
        "\n",
        "    dftrain=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/train.tsv\",sep=\"\\t\",header=None,names=cols)\n",
        "    dfval=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/val.tsv\",sep=\"\\t\",header=None,names=cols)\n",
        "    dftest=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/test.tsv\",sep=\"\\t\",header=None,names=cols)\n",
        "\n",
        "    dftrain=dftrain.loc[:,['statement','justification']]\n",
        "    dfval=dfval.loc[:,['statement','justification']]\n",
        "    dftest=dftest.loc[:,['statement','justification']]\n",
        "    dftrain=dftrain.append(dfval)\n",
        "    dftrain=dftrain.append(dftest)\n",
        "      \n",
        "    dftrain=dftrain.dropna(axis=0)\n",
        "      \n",
        "    all_statements=dftrain['statement'].values\n",
        "    all_statements=[self.clean(statement,True).split() for statement in all_statements]\n",
        "\n",
        "    all_justifications=dftrain['justification'].values\n",
        "    all_justifications=[self.clean(justification,True).split() for justification in all_justifications]\n",
        "\n",
        "    return all_statements,all_justifications\n",
        "\n",
        "\n",
        "  def clean(self,text,is_quality):\n",
        "    text=text.lower()\n",
        "    stp=set(stopwords.words(\"english\"))\n",
        "    placesp = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    removech= re.compile('[^0-9a-z #+_]')\n",
        "    st=WordNetLemmatizer()\n",
        "    text=re.sub(placesp,' ',text)\n",
        "    text=re.sub(removech,' ',text)\n",
        "\n",
        "    if is_quality == True:\n",
        "      return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    text=text.split()\n",
        "    text=[w for w in text if not w in stp]\n",
        "    text=[st.lemmatize(w) for w in text]\n",
        "    text=\" \".join(text)\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return text\n",
        "    \"\"\"\n",
        "  Transfer sentence to indeces word in Embedding\n",
        "  take text and word to index dictionary \n",
        "  return list of indeces word in Embedding\n",
        "\n",
        "  \"\"\"\n",
        "  def transfer_sent(self,text,word_to_index):\n",
        "    text=text.split(' ')\n",
        "    ret=[]\n",
        "    for w in text:\n",
        "      if w in word_to_index and w !=\"\":\n",
        "        ret.append(word_to_index[w])\n",
        "    return ret\n",
        "  \n",
        "\n",
        "  def padding_test(self,text,ln):\n",
        "    if len(text)<ln:\n",
        "      text+=[0]*(ln-len(text))\n",
        "    return text\n",
        "    \n",
        "  \"\"\"\n",
        "  Word Embeddings of words take dictionary of word to embedding and word to index\n",
        "  and return Embeddings Matrix [index,Embedding] \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def pretrained_embedding_layer(self,word_to_vec_map, word_to_index):\n",
        "      vocab_len = len(word_to_index) + 1\n",
        "      emb_matrix = np.zeros((vocab_len,300))\n",
        "      for word, index in word_to_index.items():\n",
        "          emb_matrix[index, :] = word_to_vec_map[word]\n",
        "      return emb_matrix\n",
        "\n",
        "  \"\"\"\n",
        "    Read Glove File take url of file return the two dictionaries ( word to index and word to vector in embedding )\n",
        "    and one list of index to word  \n",
        "    (glove file url) --> words_to_index, index_to_words, word_to_vec_map\n",
        "    \n",
        "  \"\"\"\n",
        "  def read_glove_vecs(self,glove_file):\n",
        "        with open(glove_file, 'r',encoding='UTF-8') as f:\n",
        "            words = set()\n",
        "            word_to_vec_map = {}\n",
        "            for line in f:\n",
        "                line = line.strip().split()\n",
        "                curr_word = line[0]\n",
        "                words.add(curr_word)\n",
        "                word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "            \n",
        "            i = 1\n",
        "            words_to_index = {}\n",
        "            index_to_words = {}\n",
        "            for w in sorted(words):\n",
        "                words_to_index[w] = i\n",
        "                index_to_words[i] = w\n",
        "                i = i + 1\n",
        "        return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "\n",
        "  def quality(self,statement,justification):\n",
        "\n",
        "    smoother = SmoothingFunction()\n",
        "    score_statement = sentence_bleu(self.all_statements, statement, smoothing_function=smoother.method2,emulate_multibleu=True)\n",
        "    score_justification = sentence_bleu(self.all_justifications, justification,smoothing_function=smoother.method2, emulate_multibleu=True)\n",
        "    return (score_statement+score_justification)/2\n",
        "\n",
        "\n",
        "  def reality(self,statement,subject,justification):\n",
        "    \n",
        "    quality_new = self.quality(self.clean(statement,True),self.clean(justification,True))\n",
        "\n",
        "    statement=self.padding_test(self.transfer_sent(self.clean(statement,False),self.word_to_index),410)\n",
        "    subject=self.padding_test(self.transfer_sent(self.clean(subject,False),self.word_to_index),30)\n",
        "    justification=self.padding_test(self.transfer_sent(self.clean(justification,False),self.word_to_index),1000)\n",
        "    \n",
        "    once=torch.tensor([statement+subject+justification]).to(self.device)\n",
        "    self.model.eval()\n",
        "    h=self.model.init_hidden()\n",
        "    pred,h=self.model(once,h)\n",
        "    return pred.item()*100,quality_new*100\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsL9VdcpZhze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "statement=\"The decade that shattered trust in politics\"\n",
        "subject=\"politics\"\n",
        "justification='''It is totally normal for ministers and officials in high pressure jobs to have quarrels and tricky conversations.\n",
        "\n",
        "Arguably, a bit of healthy tension can be a good thing for governments, to make sure that ideas are tested and policies properly thought through.\n",
        "\n",
        "It is also normal from time to time for senior officials to move quietly to different government departments if a relationship breaks down with their political boss, or sometimes, for them to retire early if the situation has become impossible.\n",
        "\n",
        "There is nothing remotely normal however about a top government official quitting their job, suing the government in the belief they were forced out, deciding to go public with the reasons, and accusing one of the most senior politicians in the country of not being straight with the truth.\n",
        "\n",
        "But that is exactly what's happened. Sir Philip Rutnam has been one of the most senior civil servants for years, in charge at the Home Office for the last few.\n",
        "\n",
        "His time there has not always been an unalloyed success - the Home Office, as one of the biggest and most complicated departments in the government, has struggled with various issues, most notably the Windrush scandal. The Home Office is often seen as a poisoned chalice given the nature of its job.'''\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnNqUC9lSifg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = preprocess()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbrJHeDoYwBB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec06cd4a-4b8c-43bd-c7c0-d8f8841cc70b"
      },
      "source": [
        "in_all = model.reality(statement,subject,justification)\n",
        "print(\"The credibility of new {:.2f}% and the quality {:.2f}%\".format(in_all[0],in_all[1]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The credibility of new 52.45% and the quality 4.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al9Lam2ZnB3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}